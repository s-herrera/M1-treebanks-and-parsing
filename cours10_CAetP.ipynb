{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus arbor√©s et parsing - 2024/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the environment\n",
    "!echo $VIRTUAL_ENV\n",
    "# and...\n",
    "!which python\n",
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy model\n",
    "# https://spacy.io/usage\n",
    "# %%python3 -m spacy download fr_core_news_sm\n",
    "# ou python au lieu de python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"frwiki_50.1000.conllu\", encoding=\"utf-8\") as file: #file is a TextIOWrapper object.\n",
    "    data1 = file.readlines() # liste de lignes\n",
    "    #data = file.read() # cha√Æne de caract√®res\n",
    "    # for line in file:\n",
    "    #     print(line)\n",
    "\n",
    "data1[:20]\n",
    "# data[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nombreuses fa√ßons de compter les phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si le conllu est bien format√©, on peut faire\n",
    "n_sentences_A = data1.count(\"\\n\")\n",
    "print(f\"{n_sentences_A=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mais ce n'est pas du toujours le cas, alors...\n",
    "n_sentences_B = 0\n",
    "for line in data1:\n",
    "    if line.startswith(\"# sent_id\"):\n",
    "        n_sentences_B += 1\n",
    "print(f\"{n_sentences_B=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut s√©parer les phrases depuis le d√©but\n",
    "\n",
    "def read_conllu(path):\n",
    "    \"\"\"\n",
    "    Transformer le fichier conllu dans une liste de listes.\n",
    "    Chaque liste est une phrase, chaque √©l√©ment est une ligne.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(path, encoding=\"utf-8\") as in_stream:\n",
    "        next(in_stream)\n",
    "        sent = []\n",
    "        for line in in_stream:\n",
    "            if line == \"\\n\":\n",
    "                data.append(sent)\n",
    "                sent = []\n",
    "                continue\n",
    "            else:\n",
    "                sent.append(line)\n",
    "    return data\n",
    "\n",
    "path = \"frwiki_50.1000.conllu\"\n",
    "data2 = read_conllu(path)\n",
    "\n",
    "for lst in data2[:1]:\n",
    "    print(lst)\n",
    "\n",
    "n_sentences_C = len(data2)\n",
    "print(f\"{n_sentences_C=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut v√©rifier les r√©sultats, m√™me si dans ce cas c'est inutile\n",
    "assert n_sentences_A == 996 and n_sentences_A == n_sentences_B and n_sentences_B == n_sentences_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un dictionnaire est peut-√™tre la meilleure fa√ßon de stocker chaque phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D'autres informations importantes, telles que les m√©tadonn√©es et les caract√©ristiques, ne sont pas prises en compte.\n",
    "\n",
    "def read_conllu(path) :\n",
    "    \"\"\"\n",
    "    Transformer le fichier conllu dans une liste de liste.\n",
    "    Chaque liste est une phrase, chaque token est un dictionnaire.\n",
    "    Certains informations importantes, telles que les m√©tadonn√©es et les caract√©ristiques, ne sont pas prises en compte.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    with open(path, encoding=\"utf-8\") as in_stream:\n",
    "        sent = []\n",
    "        for line in in_stream:\n",
    "            line = line.strip()\n",
    "            columns = line.split(\"\\t\")\n",
    "\n",
    "            if columns[0].startswith(\"#\"):\n",
    "                continue\n",
    "            if \"-\" in columns[0]:\n",
    "                continue\n",
    "            if columns[0]: # c√†d, s'il n'est pas vide\n",
    "                sent.append({\n",
    "                    \"idx\" : columns[0],\n",
    "                    \"token\" : columns[1],\n",
    "                    \"lemma\" : columns[2],\n",
    "                    \"upos\" : columns[3],\n",
    "                    \"head\" : columns[6],\n",
    "                    \"deprel\" : columns[7]})\n",
    "            else:\n",
    "                data.append(sent)\n",
    "                sent = []\n",
    "\n",
    "    return data\n",
    "\n",
    "path = \"frwiki_50.1000.conllu\"\n",
    "data = read_conllu(path)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compter le nombre des tokens et des lemmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de tokens\n",
    "n_tokens = 0\n",
    "for sentence in data:\n",
    "    n_tokens += len(sentence)\n",
    "print(n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avec une liste en compr√©hension \n",
    "sum([len(sentence) for sentence in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöß Quelle est la longueur moyenne d'une phrase dans le corpus ?\n",
    "# √Ä vous..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combien y a-t-il de tokens diff√©rents ?\n",
    "uniq_tokens = set()\n",
    "for sentence in data:\n",
    "    for tk in sentence:\n",
    "        uniq_tokens.add(tk['token'])\n",
    "\n",
    "print(f\"{len(uniq_tokens)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combien y a-t-il de lemmes diff√©rents ?\n",
    "# √Ä vous..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combien y a-t-il de lemmes diff√©rents ? R√©ponse avec une liste en compr√©hension  \n",
    "uniq_lemmas = set([tk['lemma'] for sentence in data for tk in sentence])\n",
    "print(f\"{len(uniq_lemmas)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, les compter pour de vrai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cup√©rer la liste de tous tokens\n",
    "\n",
    "lemmas = []\n",
    "for sentence in data:\n",
    "    for tk in sentence:\n",
    "        if tk['upos'] != \"PUNCT\":\n",
    "            lemmas.append(tk['lemma'])\n",
    "\n",
    "# Avec une liste en compr√©hension  \n",
    "#tokens = [tk['token'] for sentence in data for tk in sentence]\n",
    "\n",
    "counter = {}\n",
    "for l in lemmas:\n",
    "    if l in counter:\n",
    "        counter[l] += 1\n",
    "    else:\n",
    "        counter[l] = 1\n",
    "\n",
    "# On ordonne le dictionnaire et on obtient une liste de tuples ordonn√©s\n",
    "sorted_lemmas = sorted(counter.items(), key=lambda item : item[1], reverse=True)\n",
    "sorted_lemmas[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avec la meilleure biblioth√®que de Python\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(lemmas)\n",
    "sorted_lemmas = sorted(counter.items(), key=lambda item : item[1], reverse=True)\n",
    "sorted_lemmas[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "freq = []\n",
    "labels = []\n",
    "for lemma in sorted_lemmas[:20]:\n",
    "    freq.append(lemma[0])\n",
    "    labels.append(lemma[1])\n",
    "\n",
    "sns.barplot(x=freq, y=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repr√©sentez graphiquement tous les noms et leur fr√©quence avec un barplot\n",
    "# √Ä vous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repr√©sentez la distribution de partie du discours avec un barplot\n",
    "# √Ä vous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combien y a-t-il des adjectifs avant et apr√®s leur nom?\n",
    "\n",
    "adj_position = {\"before\" : 0, \"after\": 0}\n",
    "\n",
    "# for sentence in data:\n",
    "#     for token_i in sentence:\n",
    "#         if token_i['upos'] == \"NOUN\":\n",
    "#             for token_j in sentence:\n",
    "                # √Ä compl√©ter, r√©ponse en bas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_position = {\"before\" : 0, \"after\": 0}\n",
    "\n",
    "for sentence in data:\n",
    "    for token_i in sentence:\n",
    "        if token_i['upos'] == \"NOUN\":\n",
    "            for token_j in sentence:\n",
    "                if token_j['head'] == token_i['idx'] and token_j['upos'] == \"ADJ\":\n",
    "                    if int(token_i['idx']) < int(token_j['idx']):\n",
    "                        adj_position['after'] += 1\n",
    "                    else:\n",
    "                        adj_position['before'] += 1\n",
    "\n",
    "sns.barplot(adj_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et si je veux tagger et analyser syntaxiquement un texte ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation tr√®s naive\n",
    "with open(\"conte.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "sentences = text.split(\".\")\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = sentences[1].strip()\n",
    "doc = nlp(sent)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_) # token.morph, token.lemma_, token.dep_\n",
    "# upos = [(token.text, token.pos_) for token in doc]\n",
    "# print(upos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(f\"token : {token.text}\\t deprel : {token.dep_}\\t token_head : {token.head.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver un verbe avec un sujet    \n",
    "verbs = set()\n",
    "for possible_subject in doc:\n",
    "    if possible_subject.dep_ == \"nsubj\" and possible_subject.head.pos_ == \"VERB\":\n",
    "        verbs.add(possible_subject.head)\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysez avec SpaCy les phrases du conte Le Joueur de fl√ªte de Hamelin. \n",
    "# a. Combient y a-t-il de mots ?\n",
    "# b. Quelle est la phrase la plus longue ?\n",
    "# c. Quelle est la longueur moyenne de phrases?\n",
    "# d. Quel est le mot le plus longue ?\n",
    "# e. Combient y a-t-il de verbes ?\n",
    "# f. Plottez avec un barplot les dix mots les plus fr√©quents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
