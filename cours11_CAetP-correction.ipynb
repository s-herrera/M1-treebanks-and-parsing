{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus arborés et parsing - 2024/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the environment\n",
    "!echo $VIRTUAL_ENV\n",
    "# and...\n",
    "!which python\n",
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et si je veux tagger et analyser syntaxiquement un texte ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une segmentation très naive\n",
    "with open(\"conte.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "    \n",
    "#sentences = [sent.strip() for sent in text.split(\".\")]\n",
    "sentences = []\n",
    "for sent in text.split('.'):\n",
    "    sent_stripped = sent.strip()\n",
    "    if sent_stripped:\n",
    "        sentences.append(sent_stripped)\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encore mieux avec spaCy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "# Je parse tout le text au même temps\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(f\"la racine est {sent.root.text}\")\n",
    "    for token in sent:\n",
    "        print(token.text, token.pos_, token.morph, token.morph, token.lemma_, token.dep_)\n",
    "    \n",
    "    # pour trouver les enfants\n",
    "    for child in token.children:\n",
    "        print(token, child.text)\n",
    "\n",
    "# upos = [(token.text, token.pos_) for token in doc]\n",
    "# print(upos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in list(doc.sents)[0][:3]:\n",
    "    print(f\"token : {token.text}\\t deprel : {token.dep_}\\t token_head : {token.head.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver les verbes avec un sujet    \n",
    "verbs = list()\n",
    "for possible_subject in doc:\n",
    "    if possible_subject.dep_ == \"nsubj\" and possible_subject.head.pos_ == \"VERB\":\n",
    "        verbs.append(possible_subject.head)\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(list(doc.sents)[0], style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysez avec SpaCy les phrases du conte Le Joueur de flûte de Hamelin. \n",
    "# a. Combient y a-t-il de mots ?\n",
    "# b. Quelle est la phrase la plus longue ?\n",
    "# c. Quelle est la longueur moyenne de phrases?\n",
    "# d. Quel est le mot le plus longue ?\n",
    "# e. Combient y a-t-il de verbes ?\n",
    "# f. Plottez avec un barplot les dix mots les plus fréquents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_len = []\n",
    "tokens = []\n",
    "verbs = []\n",
    "\n",
    "for sent in sentences:\n",
    "    # Je parse une phrase à la fois\n",
    "    doc = nlp(sent)\n",
    "    sent_len = len(doc)\n",
    "    sentences_len.append(sent_len)\n",
    "    for token in doc:\n",
    "        tokens.append(token.text)\n",
    "        if token.pos_ == \"VERB\":\n",
    "            verbs.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tokens\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index of the longest sentence to get the right sentence string\n",
    "res = sentences_len.index(max(sentences_len))\n",
    "sentences[res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average token/sentence\n",
    "len(tokens)/len(sentences_len) # /len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One (bad) way to get te longest token\n",
    "max_word_len = 0\n",
    "for w in words:\n",
    "    if len(w) > max_word_len:\n",
    "        max_word_len = len(w)\n",
    "        res = w\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A better way\n",
    "max_word_len = 0\n",
    "res = []\n",
    "for w in words:\n",
    "    if len(w) > max_word_len:\n",
    "        max_word_len = len(w)\n",
    "        res = [w]\n",
    "    elif len(w) == max_word_len:\n",
    "        res.append(w)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of verbs\n",
    "len(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 10 most frequent tokens\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "counter = Counter(tokens)\n",
    "res = sorted(counter.items(), key= lambda x: x[1], reverse=True)\n",
    "res[:10]\n",
    "\n",
    "freq = []\n",
    "labels = []\n",
    "for tk in res[:10]:\n",
    "    freq.append(tk[0])\n",
    "    labels.append(tk[1])\n",
    "\n",
    "sns.barplot(x=freq, y=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encore à vous...\n",
    "# a. Quelle est la distribution de dépendances à gauche et à droite? Check spaCy API https://spacy.io/api/token\n",
    "# b. Quelles sont les dépendances qui, dans au moins 90 % des cas, vont à gauche ? \n",
    "# c. Quel est le nombre maximal de dépendants pour un nœud ?\n",
    "# d. Y a-t-il des dépendances non projectives ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b, and c\n",
    "\n",
    "doc = nlp(text)\n",
    "max_children = 0\n",
    "\n",
    "unique_deps = set()\n",
    "for token in doc:\n",
    "    unique_deps.add(token.dep_)\n",
    "#unique_deps = set([token.dep_ for token in doc])\n",
    "\n",
    "deps = {}\n",
    "for dep in unique_deps:\n",
    "    deps[dep] = { 'left': 0, 'right': 0 }\n",
    "#deps = {dep : { 'left': 0, 'right': 0 } for dep in unique_deps }\n",
    "# you can also do this with a defaultdict\n",
    "\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        n_children = 0\n",
    "        for child in token.children:\n",
    "            # for question c\n",
    "            n_children += 1\n",
    "\n",
    "            # the token is the head\n",
    "            if token.idx < child.idx:\n",
    "                deps[child.dep_]['right'] += 1\n",
    "            elif token.idx > child.idx:\n",
    "                deps[child.dep_]['left'] += 1\n",
    "\n",
    "        if n_children > max_children:\n",
    "            max_children = n_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a\n",
    "n_left = sum(v['left'] for v in deps.values())\n",
    "n_right = sum(v['right'] for v in deps.values())\n",
    "total = n_left + n_right\n",
    "\n",
    "print(n_left / total, n_right / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b\n",
    "mostly_left = []\n",
    "for dep, values in deps.items():\n",
    "    left = values['left']\n",
    "    right = values['right']\n",
    "    total = left + right\n",
    "    if left and left/total >= 0.9:\n",
    "        mostly_left.append(dep)\n",
    "print(mostly_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c\n",
    "max_children\n",
    "# In one line, but harder to understand\n",
    "# max(len(list(token.children)) for sent in doc.sents for token in sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
